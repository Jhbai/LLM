{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb49e6b",
   "metadata": {},
   "source": [
    "# 純文字模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fb35fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229633d2ac834e498befbc06e4606bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HybridCache, Gemma3ForCausalLM, GemmaTokenizerFast\n",
    "\n",
    "PATH = \"./gemma/gemma3_4b\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    PATH,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    "    )\n",
    "model = model.eval()\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(PATH, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62cc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"\"\"<start_of_turn>user\n",
    "[一律用繁體中文回應]{prompt}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "def gemma_resp(prompt):\n",
    "    MSG = msg.format(prompt=prompt)\n",
    "    input_ids = torch.tensor(tokenizer.encode(MSG)).to(model.device)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    past_key_values = HybridCache(\n",
    "        config = model.config,\n",
    "        max_cache_len=1024,\n",
    "        max_batch_size=1,\n",
    "        device=model.device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    cache_position = torch.arange(\n",
    "        input_ids.shape[-1], dtype=torch.long, device=model.device\n",
    "    )\n",
    "    eos_token_ids = [tokenizer.eos_token_id, 106]\n",
    "\n",
    "    output_len = 32768\n",
    "    res = list()\n",
    "    for _ in range(output_len):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "                cache_position=cache_position\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            token_id = next_token.item()\n",
    "            if token_id in eos_token_ids:\n",
    "                break\n",
    "            res += [tokenizer.decode(token_id)]\n",
    "            print(res[-1], end=\"\", flush=True)\n",
    "            input_ids = next_token\n",
    "            attention_mask = None\n",
    "            cache_position = cache_position[-1:] + 1\n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d65e4a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一個大型語言模型，可以做很多事情呢！以下是一些我能做到的：\n",
      "\n",
      "**1. 回答你的問題：**\n",
      "\n",
      "*   **知識查詢：** 只要你問我任何問題，我都會盡力用我所學的知識來回答你。無論是歷史、科學、文化、還是其他任何領域，我都可以提供資訊。\n",
      "*   **解釋複雜概念：** 如果你對某個概念感到困惑，我可以試著用簡單易懂的方式來解釋它。\n",
      "*   **提供定義：** 只要你告訴我一個詞語，我就可以提供它的定義和解釋。\n",
      "\n",
      "**2. 創作內容：**\n",
      "\n",
      "*   **寫作：** 我可以幫你寫各種文章，例如：\n",
      "    *   故事\n",
      "    *   詩歌\n",
      "    *   文章\n",
      "    *   信件\n",
      "    *   電子郵件\n",
      "    *   廣告文案\n",
      "*   **翻譯：** 我可以將文字從一種語言翻譯成另一種語言。\n",
      "*   **摘要：** 我可以將長篇文章或段落簡短地總結。\n",
      "*   **生成不同風格的文本：** 例如，我可以模仿莎士比亞的風格寫一首關於貓的詩，或者用現代口語寫一篇關於科技的文章。\n",
      "\n",
      "**3. 協助你完成任務：**\n",
      "\n",
      "*   **編寫程式碼：** 我可以幫你寫一些簡單的程式碼，例如 Python、JavaScript 等。\n",
      "*   **解決數學問題：** 我可以解決一些簡單的數學問題。\n",
      "*   **生成列表：** 我可以根據你的要求生成列表，例如購物清單、待辦事項清單等。\n",
      "*   **提供建議：** 我可以根據你的描述，提供一些建議，例如旅遊建議、美食推薦等。\n",
      "\n",
      "**4. 娛樂你：**\n",
      "\n",
      "*   **講笑話：** 我可以講一些笑話逗你開心。\n",
      "*   **玩文字遊戲：** 我可以和你一起玩一些文字遊戲。\n",
      "*   **創作故事：** 我可以根據你的要求創作故事。\n",
      "\n",
      "**5. 其他：**\n",
      "\n",
      "*   **角色扮演：** 我可以扮演不同的角色，與你進行互動。\n",
      "*   **提供創意：** 如果你卡住了，我可以提供一些創意，幫助你解決問題。\n",
      "*   **學習：** 我可以根據你的提問，學習新的知識。\n",
      "\n",
      "**請記住：** 我是一個語言模型，我的知識是基於我訓練時所學習的數據。我可能會犯錯，所以請務必對我的回答進行核實。\n",
      "\n",
      "**現在，你想讓我做什麼呢？ 告訴我你的需求吧！**\n"
     ]
    }
   ],
   "source": [
    "res = gemma_resp(\"你能夠做什麼事情呢？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd021fd",
   "metadata": {},
   "source": [
    "# 多模態模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ed967e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c88fae32ca4c4ca8d0b2c98e704ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, HybridCache, Gemma3ForConditionalGeneration, Cache\n",
    "from PIL import Image\n",
    "PATH = \"./gemma/gemma3_4b\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    PATH,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    "    )\n",
    "model = model.eval()\n",
    "processor = AutoProcessor.from_pretrained(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32cb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "msg = \"<start_of_turn>user\\n{prompt}\\n<end_of_turn><start_of_image>\\n<start_of_turn>model\"\n",
    "\n",
    "def gemma_resp(prompt, image_path):\n",
    "    # 讀圖片 + 縮放\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt = msg.format(prompt=prompt)\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    pixel_values = inputs[\"pixel_values\"]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    # cache_position = torch.tensor([input_ids.shape[-1]], device=model.device)\n",
    "    seq_len = input_ids.shape[-1]\n",
    "    cache_position = torch.arange(seq_len, device=model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            use_cache=True,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    past_key_values = outputs.past_key_values\n",
    "    next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "    input_ids = next_token\n",
    "\n",
    "    eos_token_ids = {processor.tokenizer.eos_token_id, 106}\n",
    "\n",
    "    output_len = 32768\n",
    "    res = list()\n",
    "    current_pos = seq_len\n",
    "    for _ in range(output_len):\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(1, 1, device=model.device, dtype=torch.long)], dim=-1)\n",
    "        cache_position = torch.tensor([current_pos], device=model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "                cache_position=cache_position,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=None\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            token_id = next_token.item()\n",
    "            if token_id in eos_token_ids:\n",
    "                break\n",
    "            \n",
    "            res += [processor.decode(token_id, skip_special_tokens=True)]\n",
    "            print(res[-1], end=\"\", flush=True)\n",
    "            input_ids = next_token\n",
    "            current_pos += 1\n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5afaaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這張圖中，我能清楚地看到一個女孩（一位）。 雖然畫面中有許多坦克和巨大的機器人，但只有一個明顯的人。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'這張圖中，我能清楚地看到一個女孩（一位）。 雖然畫面中有許多坦克和巨大的機器人，但只有一個明顯的人。\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_resp(\"[繁體中文回答] 這張圖有幾個人 ?\", \"C:/Users/jhbai/Pictures/CUTE/image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79830ddc",
   "metadata": {},
   "source": [
    "# Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef90dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ddeeb1ebca4494b5c6b1e785badaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HybridCache, Gemma3ForCausalLM, GemmaTokenizerFast\n",
    "\n",
    "PATH = \"./gemma/gemma3_4b\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    PATH,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    "    )\n",
    "model = model.eval()\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(PATH, truncate = True)\n",
    "\n",
    "# 新增 padding_side='left'，因為在生成任務中，將填充放在左側更為常見且高效\n",
    "tokenizer.padding_side = 'left'\n",
    "# 如果 tokenizer 沒有預設的 pad_token，通常會將其設定為 eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "msg = \"\"\"<start_of_turn>user\n",
    "[一律用繁體中文回應]{prompt}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af9fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemma_resp(prompts: list[str]):\n",
    "    batch_size = len(prompts)\n",
    "    formatted_prompts = [msg.format(prompt=p) for p in prompts]\n",
    "    inputs = tokenizer(formatted_prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    past_key_values = HybridCache(\n",
    "        config = model.config,\n",
    "        max_cache_len=1024,\n",
    "        max_batch_size=batch_size,\n",
    "        device=model.device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    # 不做 cache_position，避免與 padding 衝突\n",
    "    eos_token_ids = [tokenizer.eos_token_id, 106]\n",
    "\n",
    "    output_len = 32768\n",
    "    res = [[] for _ in range(batch_size)]\n",
    "    is_eos = torch.zeros(batch_size, dtype=torch.bool, device=model.device)\n",
    "\n",
    "    for _ in range(output_len):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                if not is_eos[i]:\n",
    "                    token_id = next_token[i].item()\n",
    "                    if token_id in eos_token_ids:\n",
    "                        is_eos[i] = True \n",
    "                    else:\n",
    "                        decoded_token = tokenizer.decode(token_id)\n",
    "                        res[i].append(decoded_token)\n",
    "\n",
    "            if is_eos.all():\n",
    "                break\n",
    "            \n",
    "            input_ids = next_token\n",
    "            # 在每次迭代時，將 attention_mask 擴展一個單位 (長度為1，值為1)，新 token 需要被計算\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones((batch_size, 1), dtype=torch.long, device=model.device)], dim=1)\n",
    "\n",
    "    final_outputs = [\"\".join(r) for r in res]\n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "465b609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: [繁體中文回答]一句話說明什麼是Martingale\n",
      "Response: 馬丁格拉策略是一種賭博策略，它主張在輸局後，將下注金額翻倍，直到贏錢，以此來彌補之前的損失。\n",
      "\n",
      "Prompt: [繁體中文回答]一句話解釋Bolzano-Wierstrass Theorem\n",
      "Response: 波茲瑪-魏爾斯特拉斯定理指出，任何連續函數，在閉合區間內都必有最大值和最小值。\n",
      "\n",
      "Prompt: [繁體中文回答]一句話描述Compiler的運作精神\n",
      "Response: 編譯器就像一位翻譯家，將高階程式碼轉譯成電腦能理解的低階指令。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 使用範例 ---\n",
    "prompts_to_run = [\"[繁體中文回答]一句話說明什麼是Martingale\", \"[繁體中文回答]一句話解釋Bolzano-Wierstrass Theorem\", \"[繁體中文回答]一句話描述Compiler的運作精神\"]\n",
    "responses = gemma_resp(prompts_to_run)\n",
    "for i, (prompt, response) in enumerate(zip(prompts_to_run, responses)):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afead345",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7508d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1516e7d60bff49ddacb64ee030f16a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HybridCache, Gemma3ForCausalLM, GemmaTokenizerFast\n",
    "\n",
    "PATH = \"./gemma/gemma3_4b\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    PATH,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    output_hidden_states=True  # 隱藏狀態\n",
    "    )\n",
    "model = model.eval()\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(PATH, truncate = True)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "msg = \"\"\"<start_of_turn>user\n",
    "[一律用繁體中文回應]{prompt}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5995f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemma_resp(prompts: list[str]):\n",
    "    batch_size = len(prompts)\n",
    "    formatted_prompts = [msg.format(prompt=p) for p in prompts]\n",
    "    inputs = tokenizer(formatted_prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    past_key_values = HybridCache(\n",
    "        config = model.config,\n",
    "        max_cache_len=1024,\n",
    "        max_batch_size=batch_size,\n",
    "        device=model.device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    # 不做 cache_position，避免與 padding 衝突\n",
    "    eos_token_ids = [tokenizer.eos_token_id, 106]\n",
    "\n",
    "    output_len = 32768\n",
    "    res = [[] for _ in range(batch_size)]\n",
    "    is_eos = torch.zeros(batch_size, dtype=torch.bool, device=model.device)\n",
    "\n",
    "    for _ in range(output_len):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                if not is_eos[i]:\n",
    "                    token_id = next_token[i].item()\n",
    "                    if token_id in eos_token_ids:\n",
    "                        is_eos[i] = True \n",
    "                    else:\n",
    "                        decoded_token = tokenizer.decode(token_id)\n",
    "                        res[i].append(decoded_token)\n",
    "\n",
    "            if is_eos.all():\n",
    "                break\n",
    "            \n",
    "            input_ids = next_token\n",
    "            # 在每次迭代時，將 attention_mask 擴展一個單位 (長度為1，值為1)，新 token 需要被計算\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones((batch_size, 1), dtype=torch.long, device=model.device)], dim=1)\n",
    "\n",
    "    final_outputs = [\"\".join(r) for r in res]\n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2816229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: list[str]):\n",
    "    original_padding_side = tokenizer.padding_side\n",
    "    tokenizer.padding_side = 'right'\n",
    "    inputs = tokenizer(\n",
    "        texts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=32768\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "    # Mean Pooling\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    expanded_mask = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float() # (batch_size, seq_len, hidden_size) -> (batch_size, seq_len, 1)\n",
    "    sum_embeddings = torch.sum(last_hidden_states * expanded_mask, 1)\n",
    "    sum_mask = torch.clamp(expanded_mask.sum(1), min=1e-9)\n",
    "    mean_pooled_embeddings = sum_embeddings / sum_mask\n",
    "    normalized_embeddings = F.normalize(mean_pooled_embeddings, p=2, dim=1)\n",
    "\n",
    "    tokenizer.padding_side = original_padding_side\n",
    "    return normalized_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e2e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 搜尋結果 (最相關的前 3 筆) ---\n",
      "機器人喜歡吃蔬菜 tensor(0.9360, device='cuda:0')\n",
      "機器人的薪水買不起房 tensor(0.9283, device='cuda:0')\n",
      "機器人每天都會去爬山 tensor(0.9228, device='cuda:0')\n",
      "====================AI回應********************\n",
      "根據提供的資訊，機器人的興趣是：\n",
      "\n",
      "*   **喜歡吃蔬菜**\n",
      "*   **每天都會去爬山**\n",
      "\n",
      "這兩個都是直接提到的興趣。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1. 假設這是您的知識庫 (Knowledge Base)\n",
    "    documents = [\n",
    "        \"機器人喜歡吃蔬菜\",\n",
    "        \"機器人每天都會去爬山\",\n",
    "        \"工程師最喜歡喝奶茶\",\n",
    "        \"黑貓嚕嚕是我的本名\",\n",
    "        \"機器人的薪水買不起房\",\n",
    "        \"機器人討厭吃芭樂\"\n",
    "    ]\n",
    "    doc_embeddings = get_embeddings(documents)\n",
    "\n",
    "    query = \"機器人的興趣是什麼？\"\n",
    "    query_embedding = get_embeddings([query])\n",
    "    cosine_scores = F.cosine_similarity(query_embedding, doc_embeddings, dim=1)\n",
    "\n",
    "    top_k = 3\n",
    "    top_results = torch.topk(cosine_scores, k=min(top_k, len(documents)))\n",
    "\n",
    "    print(\"\\n--- 搜尋結果 (最相關的前 {} 筆) ---\".format(top_k))\n",
    "    for score, idx in zip(top_results.values, top_results.indices):\n",
    "        print(documents[idx], score)\n",
    "    print(\"=\"*20 + \"AI回應\" + \"*\"*20)\n",
    "\n",
    "    retrieved_doc = [documents[i] for i in top_results.indices]\n",
    "    qa_prompt = f\"\"\"\n",
    "    根據以下資訊：\n",
    "    \"{retrieved_doc}\"\n",
    "    \n",
    "    請回答這個問題：\"{query}\"\n",
    "    \"\"\"\n",
    "    answer = gemma_resp([qa_prompt])\n",
    "    print(answer[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
