{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d9fab5",
   "metadata": {},
   "source": [
    "# Gemma3N-E2N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe9d57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\dev\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HybridCache, Gemma3ForCausalLM, GemmaTokenizerFast, AutoProcessor, Gemma3nForCausalLM\n",
    "# transformers >= 4.53.0\n",
    "# timm==1.0.19\n",
    "# torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "PATH = \"C:/Users/user/LLM/gemma3n\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "# model = Gemma3ForCausalLM.from_pretrained(\n",
    "#     PATH,\n",
    "#     quantization_config=quantization_config,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     low_cpu_mem_usage=True\n",
    "#     )\n",
    "\n",
    "model = Gemma3nForCausalLM.from_pretrained(PATH,\n",
    "                                        device_map=\"auto\", \n",
    "                                        torch_dtype=torch.bfloat16,\n",
    "                                        low_cpu_mem_usage=True\n",
    "                                        )\n",
    "\n",
    "model = model.eval()\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be8c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"\"\"<start_of_turn>user\n",
    "[一律用繁體中文回應]{prompt}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "def gemma3_resp(prompt):\n",
    "    MSG = msg.format(prompt=prompt)\n",
    "    input_ids = torch.tensor(tokenizer.encode(MSG)).to(model.device)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    past_key_values = HybridCache(\n",
    "        config = model.config,\n",
    "        max_cache_len=1024,\n",
    "        max_batch_size=1,\n",
    "        device=model.device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    eos_token_ids = [tokenizer.eos_token_id, 106]\n",
    "\n",
    "    output_len = 32768\n",
    "    res = list()\n",
    "    for _ in range(output_len):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            token_id = next_token.item()\n",
    "            if token_id in eos_token_ids:\n",
    "                break\n",
    "            res += [tokenizer.decode(token_id)]\n",
    "            print(res[-1], end=\"\", flush=True)\n",
    "            input_ids = next_token\n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36578109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一個大型語言模型，由 Google DeepMind 訓練。我可以執行各種任務，包括：\n",
      "\n",
      "*   **生成不同創意文本格式的內容：** 例如詩歌、程式碼、劇本、音樂作品、電子郵件、信件等。我會盡力滿足你的所有要求。\n",
      "*   **回答你的問題，即使是開放式、具有挑戰性或奇怪的問題。** 我會盡我所能提供有用的資訊。\n",
      "*   **翻譯語言。**\n",
      "*   **總結文本。**\n",
      "*   **遵循你的指示並完成你的請求，我會盡力按照你的要求執行。**\n",
      "\n",
      "我還在不斷學習和改進中！\n",
      "\n",
      "總而言之，我可以幫助你完成各種文字相關的任務。 你想讓我做些什麼呢？\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "msg = \"\"\"<start_of_turn>user\n",
    "{prompt}<end_of_turn><eos>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "prompt = \"你可以做什麼事情？\"\n",
    "try:\n",
    "    MSG = msg.format(prompt = prompt)\n",
    "    input_ids = tokenizer.encode(MSG, return_tensors=\"pt\").to(model.device)\n",
    "    eos_token_ids = {tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")}\n",
    "    res = list()\n",
    "    past_key_values = HybridCache(\n",
    "        config = model.config,\n",
    "        max_cache_len=32796,\n",
    "        max_batch_size=1,\n",
    "        device=model.device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for _ in range(32796):\n",
    "            outputs = model(input_ids=input_ids, use_cache=True, past_key_values=past_key_values)\n",
    "            next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            token_id = next_token.item()\n",
    "            if token_id in eos_token_ids:\n",
    "                break\n",
    "            input_ids = next_token\n",
    "            # input_ids = token_id\n",
    "            res += [tokenizer.decode(token_id)]\n",
    "            print(res[-1], end=\"\", flush = True)\n",
    "\n",
    "    \n",
    "finally:\n",
    "    del input_ids\n",
    "    del outputs\n",
    "    del next_token\n",
    "    del token_id\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3c62d",
   "metadata": {},
   "source": [
    "# Gemma3N模型多模態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d0c3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\dev\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import ImageGrab\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, HybridCache, Gemma3nForConditionalGeneration, Cache\n",
    "\n",
    "PATH = \"C:/Users/user/LLM/gemma3n\"\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     )\n",
    "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
    "    PATH,\n",
    "    # quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    "    )\n",
    "model = model.eval()\n",
    "processor = AutoProcessor.from_pretrained(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae728a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def gemma_resp(prompt):\n",
    "    image = ImageGrab.grab().convert(\"RGB\")\n",
    "    prompt = processor.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\",  \"content\": [{\"type\": \"image\", \"image\": image},\n",
    "                                        {\"type\": \"text\", \"text\": prompt}]}\n",
    "        ]\n",
    "    )\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    pixel_values = inputs[\"pixel_values\"].to(dtype=model.dtype)\n",
    "    seq_len = input_ids.shape[-1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    past_key_values = outputs.past_key_values\n",
    "    next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "    input_ids = next_token\n",
    "\n",
    "    eos_token_ids = {processor.tokenizer.eos_token_id, 106}\n",
    "\n",
    "    output_len = 32768\n",
    "    res = list()\n",
    "    current_pos = seq_len\n",
    "    for _ in range(output_len):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "                pixel_values=None\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            token_id = next_token.item()\n",
    "            if token_id in eos_token_ids:\n",
    "                break\n",
    "            \n",
    "            res += [processor.decode(token_id, skip_special_tokens=True)]\n",
    "            print(res[-1], end=\"\", flush=True)\n",
    "            input_ids = next_token\n",
    "            current_pos += 1\n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6bf827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "張圖顯示了一個 Python 程式碼編輯器，程式碼正在執行一個使用 `transformers` 庫的程式。程式碼似乎正在使用一個名為 `LLM_Code` 的模型，並進行一些與生成文本相關的操作。\n",
      "\n",
      "以下是程式碼中一些關鍵部分的解釋：\n",
      "\n",
      "* **`model = model.eval()`**: 這行程式碼將模型設定為評估模式，這意味著它將不執行任何訓練相關的操作。\n",
      "* **`processor = AutoProcessor.from_pretrained(\"path/to/model\")`**: 這行程式碼從指定的目錄中載入一個 `AutoProcessor` 物件，用於處理輸入文本。\n",
      "* **`prompt = \"user: [text], [type], [image], [image], [text], [text], prompt\"`**: 這行程式碼定義了一個提示，用於指示模型生成文本。\n",
      "* **`inputs = processor(prompt, image_images=True, return_tensors=\"pt\")`**: 這行程式碼將提示和圖像作為輸入傳遞給 `processor`，並將輸出轉換為 PyTorch  tensors。\n",
      "* **`outputs = model.generate(...)`**: 這行程式碼使用模型生成文本。\n",
      "* **`with torch.no_grad(): ...`**: 這行程式碼禁用梯度計算，以提高程式碼的效率。\n",
      "\n",
      "總體而言，這段程式碼正在使用一個大型語言模型生成文本，並使用圖像作為輸入。"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'張圖顯示了一個 Python 程式碼編輯器，程式碼正在執行一個使用 `transformers` 庫的程式。程式碼似乎正在使用一個名為 `LLM_Code` 的模型，並進行一些與生成文本相關的操作。\\n\\n以下是程式碼中一些關鍵部分的解釋：\\n\\n* **`model = model.eval()`**: 這行程式碼將模型設定為評估模式，這意味著它將不執行任何訓練相關的操作。\\n* **`processor = AutoProcessor.from_pretrained(\"path/to/model\")`**: 這行程式碼從指定的目錄中載入一個 `AutoProcessor` 物件，用於處理輸入文本。\\n* **`prompt = \"user: [text], [type], [image], [image], [text], [text], prompt\"`**: 這行程式碼定義了一個提示，用於指示模型生成文本。\\n* **`inputs = processor(prompt, image_images=True, return_tensors=\"pt\")`**: 這行程式碼將提示和圖像作為輸入傳遞給 `processor`，並將輸出轉換為 PyTorch  tensors。\\n* **`outputs = model.generate(...)`**: 這行程式碼使用模型生成文本。\\n* **`with torch.no_grad(): ...`**: 這行程式碼禁用梯度計算，以提高程式碼的效率。\\n\\n總體而言，這段程式碼正在使用一個大型語言模型生成文本，並使用圖像作為輸入。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_resp(\"[繁體中文回答] 這張圖內容是什麼 ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e880fde3",
   "metadata": {},
   "source": [
    "# Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788a41fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('C:/Users/user/LLM/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d5c30",
   "metadata": {},
   "source": [
    "# Small Gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5b0a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\dev\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HybridCache, Gemma3ForCausalLM, GemmaTokenizerFast\n",
    "\n",
    "PATH = \"C:/Users/user/LLM/smallgemma3\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    PATH,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    "    )\n",
    "model = model.eval()\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(PATH, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4fdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gemma_resp(prompt, repetition_penalty=1.2): # 1. 新增 repetition_penalty 參數\n",
    "    MSG = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    input_ids = torch.tensor(tokenizer.encode(MSG)).to(model.device)\n",
    "    \n",
    "    generated_token_ids = input_ids.tolist()\n",
    "\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    past_key_values = HybridCache(\n",
    "        config=model.config,\n",
    "        max_cache_len=1024,\n",
    "        max_batch_size=1,\n",
    "        device=model.device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    eos_token_ids = [tokenizer.eos_token_id] + tokenizer.encode('<end_of_turn>')\n",
    "\n",
    "    output_len = 32768\n",
    "    res = list()\n",
    "    for _ in range(output_len):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "\n",
    "            # --- Repetition Penalty 核心邏輯開始 ---\n",
    "            if repetition_penalty != 1.0 and len(generated_token_ids) > 0:\n",
    "                unique_generated_ids = torch.tensor(\n",
    "                    list(set(generated_token_ids)), \n",
    "                    device=model.device\n",
    "                )\n",
    "                \n",
    "                # 對這些 token ID 對應的 logits 施加懲罰\n",
    "                # 分數 > 0 時，除以 penalty (降低概率)\n",
    "                # 分數 < 0 時，乘以 penalty (使其更負，進一步降低概率)\n",
    "                # 這裡使用 gather 和 scatter_ 來高效地操作\n",
    "                score = torch.gather(next_token_logits, 1, unique_generated_ids.unsqueeze(0))\n",
    "                \n",
    "                score[score > 0] /= repetition_penalty\n",
    "                score[score < 0] *= repetition_penalty\n",
    "\n",
    "                next_token_logits.scatter_(1, unique_generated_ids.unsqueeze(0), score)\n",
    "            # --- Repetition Penalty 核心邏輯結束 ---\n",
    "            \n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            token_id = next_token.item()\n",
    "            if token_id in eos_token_ids:\n",
    "                break\n",
    "            generated_token_ids.append(token_id)\n",
    "            res += [tokenizer.decode(token_id)]\n",
    "            print(res[-1], end=\"\", flush=True)\n",
    "            \n",
    "            input_ids = next_token\n",
    "    return \"\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI 旨在提供一個簡單、快速且高效的 API 接口，它主要集中在以下幾個核心功能：\n",
      "\n",
      "*   **簡單的 API 接口:**  這就是 FastAPI 的核心。它允許開發者簡單地測試和使用，並在短時間內快速測試。\n",
      "*   **快速的 Stream Processing:**  Stream Processing 整合了 Stream 處理技術，可以快速地將數據轉換為 Stream 格式，然後"
     ]
    }
   ],
   "source": [
    "question = \"幫我思考一下，想要做FastAPI的設計，實現串流效果？\"\n",
    "res = gemma_resp(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3be80d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
